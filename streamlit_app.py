# streamlit_app.py
import streamlit as st
import google.generativeai as genai

# Import c√°c module ƒë√£ t√°ch
from config import load_api_key, configure_gemini
from ui import render_sidebar, display_messages
from chatbot_logic import load_gemini_model, initialize_chat_session, generate_response_stream, get_blocked_reason

# --- 1. C·∫•u h√¨nh trang & API Key ---
st.set_page_config(page_title="Modular Chatbot V1.1", page_icon="üß©", layout="wide")
st.title("üß© Modular Gemini Chatbot")
st.caption("Chatbot ƒë∆∞·ª£c t·ªï ch·ª©c th√†nh nhi·ªÅu file")

api_key = load_api_key()
configure_gemini(api_key)

# --- 2. Render Sidebar & L·∫•y C·∫•u h√¨nh ---
render_sidebar() # H√†m n√†y c·∫≠p nh·∫≠t session_state

# L·∫•y c√°c gi√° tr·ªã c·∫•u h√¨nh t·ª´ session_state (do sidebar c·∫≠p nh·∫≠t)
selected_model_name = st.session_state.selected_model
temperature = st.session_state.temperature
max_tokens = st.session_state.max_tokens

# --- 3. Load Model & Kh·ªüi t·∫°o Chat ---
# Model ƒë∆∞·ª£c cache, ch·ªâ load l·∫°i n·∫øu selected_model_name thay ƒë·ªïi
model = load_gemini_model(selected_model_name)

# Kh·ªüi t·∫°o chat session, h√†m n√†y s·∫Ω ki·ªÉm tra v√† t·∫°o m·ªõi n·∫øu c·∫ßn
chat = initialize_chat_session(model)

# --- 4. Hi·ªÉn th·ªã L·ªãch s·ª≠ Chat ---
display_messages()

# --- 5. X·ª≠ l√Ω Input & G·ªçi API ---
if prompt := st.chat_input("üí¨ Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    # Th√™m tin nh·∫Øn ng∆∞·ªùi d√πng v√†o state v√† hi·ªÉn th·ªã
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="üë§"):
        st.markdown(prompt)

    # Chu·∫©n b·ªã g·ªçi API
    generation_config = genai.types.GenerationConfig(
        temperature=temperature,
        max_output_tokens=max_tokens
    )
    safety_settings = [
        {"category": c, "threshold": "BLOCK_NONE"} for c in [
            "HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH",
            "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT",
        ]
    ]

    # Hi·ªÉn th·ªã placeholder v√† g·ªçi h√†m t·∫°o response stream
    with st.chat_message("assistant", avatar="‚ôä"):
        message_placeholder = st.empty()
        message_placeholder.markdown("‚ôä ƒêang suy nghƒ©...")
        full_response = ""
        assistant_response_error = False # C·ªù ƒë·ªÉ ki·ªÉm tra l·ªói trong stream

        # S·ª≠ d·ª•ng generator t·ª´ chatbot_logic
        response_generator = generate_response_stream(
            chat, prompt, generation_config, safety_settings
        )

        for chunk in response_generator:
            if isinstance(chunk, str) and chunk.startswith("‚ö†Ô∏è **L·ªói:**"): # Ki·ªÉm tra n·∫øu chunk l√† th√¥ng b√°o l·ªói t·ª´ generator
                full_response = chunk # Ghi ƒë√® b·∫±ng th√¥ng b√°o l·ªói
                assistant_response_error = True
                break # D·ª´ng x·ª≠ l√Ω stream n·∫øu c√≥ l·ªói
            elif hasattr(chunk, 'text') and chunk.text:
                full_response += chunk.text
                message_placeholder.markdown(full_response + "‚ñå")
            # C√≥ th·ªÉ th√™m x·ª≠ l√Ω cho c√°c lo·∫°i chunk kh√°c n·∫øu c·∫ßn (v√≠ d·ª•: function calls)

        # X·ª≠ l√Ω sau khi stream k·∫øt th√∫c ho·∫∑c b·ªã l·ªói
        if not full_response and not assistant_response_error: # Stream xong nh∆∞ng kh√¥ng c√≥ text V√Ä kh√¥ng ph·∫£i l·ªói t·ª´ generator
            block_reason = get_blocked_reason(chat)
            full_response = f"‚ö†Ô∏è R·∫•t ti·∫øc, kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi. L√Ω do c√≥ th·ªÉ l√†: **{block_reason}**."
            assistant_response_error = True # ƒê√°nh d·∫•u l√† c√≥ v·∫•n ƒë·ªÅ

        # Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi c√πng
        message_placeholder.markdown(full_response)

        # Ch·ªâ th√™m v√†o l·ªãch s·ª≠ n·∫øu kh√¥ng c√≥ l·ªói V√Ä c√≥ n·ªôi dung
        if not assistant_response_error and full_response:
            st.session_state.messages.append({"role": "model", "content": full_response})
        elif assistant_response_error:
             # N·∫øu c√≥ l·ªói, v·∫´n th√™m tin nh·∫Øn l·ªói v√†o l·ªãch s·ª≠ ƒë·ªÉ ng∆∞·ªùi d√πng bi·∫øt
             # Ki·ªÉm tra xem tin nh·∫Øn cu·ªëi c√≥ ph·∫£i user kh√¥ng
             if st.session_state.messages[-1]["role"] == "user":
                 st.session_state.messages.append({"role": "model", "content": full_response})
             else: # N·∫øu tin nh·∫Øn cu·ªëi l√† l·ªói c≈©, c·∫≠p nh·∫≠t n√≥
                 st.session_state.messages[-1] = {"role": "model", "content": full_response}