# streamlit_app.py (Ho√†n ch·ªânh - S·ª≠ d·ª•ng Bi·∫øn m√¥i tr∆∞·ªùng / Secrets)

import streamlit as st
import os
import google.generativeai as genai # Th∆∞ vi·ªán g·ªëc Google
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.core.llms.custom import CustomLLM # L·ªõp c∆° s·ªü cho LLM t√πy ch·ªânh
from llama_index.core.embeddings import BaseEmbedding # L·ªõp c∆° s·ªü cho Embedding t√πy ch·ªânh
from llama_index.core.llms.llm import LLM
from typing import Any, List, Optional, Sequence, AsyncGenerator
from llama_index.core.callbacks import CallbackManager
from llama_index.core.llms import CompletionResponse, CompletionResponseGen, LLMMetadata
from llama_index.core.schema import BaseNode
# from tqdm import tqdm # B·ªè tqdm

# --- C·∫•u h√¨nh trang Streamlit ---
st.set_page_config(page_title="Chatbot Gemini", page_icon="‚ôä", layout="centered")
st.title("‚ôä Chatbot LlamaIndex & Google Gemini")
st.caption("ƒê∆∞·ª£c x√¢y d·ª±ng b·∫±ng LlamaIndex v√† Streamlit")

# --- Google API Key Configuration ---

google_api_key = None
secrets_available = hasattr(st, 'secrets')

# 1. Th·ª≠ l·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng tr∆∞·ªõc (ph√π h·ª£p khi ch·∫°y local)
google_api_key = os.environ.get("GOOGLE_API_KEY")
if google_api_key:
    # st.info("ƒê√£ l·∫•y API Key t·ª´ bi·∫øn m√¥i tr∆∞·ªùng.", icon="üñ•Ô∏è") # B·ªè comment n·∫øu mu·ªën x√°c nh·∫≠n
    pass # ƒê√£ l·∫•y ƒë∆∞·ª£c t·ª´ env var, kh√¥ng c·∫ßn l√†m g√¨ th√™m
elif secrets_available:
    # 2. N·∫øu kh√¥ng c√≥ bi·∫øn m√¥i tr∆∞·ªùng, th·ª≠ l·∫•y t·ª´ Streamlit Secrets (khi deploy)
    google_api_key = st.secrets.get("GOOGLE_API_KEY", None) # D√πng get v·ªõi default None
    if google_api_key:
        st.info("ƒê√£ l·∫•y API Key t·ª´ Streamlit Secrets.", icon="üîí") # Th√¥ng b√°o khi l·∫•y t·ª´ secrets

# N·∫øu sau c·∫£ hai b∆∞·ªõc tr√™n v·∫´n kh√¥ng c√≥ key
if not google_api_key:
    st.error("Vui l√≤ng cung c·∫•p Google API Key qua bi·∫øn m√¥i tr∆∞·ªùng (khi ch·∫°y local) ho·∫∑c c·∫•u h√¨nh Secrets (khi deploy).", icon="üö®")
    # Kh√¥ng cho nh·∫≠p tr·ª±c ti·∫øp n·ªØa ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n khi deploy
    # google_api_key = st.text_input("Nh·∫≠p Google API Key:", type="password", key="api_key_input")
    st.stop() # D·ª´ng ·ª©ng d·ª•ng n·∫øu kh√¥ng c√≥ key

# --- C·∫•u h√¨nh th∆∞ vi·ªán Google ---
try:
    genai.configure(api_key=google_api_key)
except Exception as e:
    st.error(f"L·ªói c·∫•u h√¨nh th∆∞ vi·ªán Google: {e}", icon="üî•")
    st.stop()

# --- Cache Models v√† Kh·ªüi t·∫°o ---
@st.cache_data
def load_available_models():
    llm_models, embed_models = [], []
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods: llm_models.append(m.name)
            if 'embedContent' in m.supported_generation_methods: embed_models.append(m.name)
    except Exception as e:
        st.warning(f"Kh√¥ng th·ªÉ li·ªát k√™ models: {e}. D√πng m·∫∑c ƒë·ªãnh.")
        llm_models, embed_models = ["models/gemini-1.5-flash-latest"], ["models/embedding-001"]
    print(f"Available LLM models: {llm_models}") # V·∫´n in ra console ƒë·ªÉ debug
    print(f"Available Embedding models: {embed_models}")
    return llm_models, embed_models

available_llm_models, available_embedding_models = load_available_models()

# --- ƒê·ªãnh nghƒ©a L·ªõp LLM T√πy ch·ªânh ---
class MyGeminiLLM(CustomLLM):
    model_name: str = "models/gemini-1.5-flash-latest"
    _model: Any = None
    def __init__(self, model_name: str = "models/gemini-1.5-flash-latest", **kwargs) -> None:
        super().__init__(**kwargs)
        self.model_name = model_name
        try:
            if self.model_name not in available_llm_models:
                 print(f"Warning: Model '{self.model_name}' not in available LLMs. Trying fallback.")
                 if available_llm_models: self.model_name = available_llm_models[0]; print(f"Switched to: {self.model_name}")
                 else: raise ValueError("No fallback LLM available.")
            self._model = genai.GenerativeModel(self.model_name)
            print(f"MyGeminiLLM initialized with model: {self.model_name}")
        except Exception as e: print(f"Error initializing LLM '{self.model_name}': {e}"); raise
    @property
    def metadata(self) -> LLMMetadata:
        context_window = 1048576 if "1.5" in self.model_name else 30720
        return LLMMetadata(context_window=context_window, num_output=8192, model_name=self.model_name, is_chat_model=True)
    def _generate_sync(self, prompt: str, **kwargs) -> CompletionResponse:
         try:
             safety_settings=[{"category": c, "threshold": "BLOCK_NONE"} for c in ["HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH", "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT"]]
             response = self._model.generate_content(prompt, safety_settings=safety_settings)
             generated_text = ""; candidate = None
             if hasattr(response, 'candidates') and response.candidates: candidate = response.candidates[0]
             if candidate and candidate.finish_reason == 1 and hasattr(candidate.content, 'parts') and candidate.content.parts: generated_text = "".join(part.text for part in candidate.content.parts if hasattr(part, 'text'))
             elif candidate and candidate.finish_reason != 0: reason = genai.types.Candidate.FinishReason(candidate.finish_reason).name; print(f"Warning: Response blocked: {reason}"); generated_text = f"[Blocked: {reason}]"
             elif hasattr(response, 'text'): generated_text = response.text
             elif hasattr(response, 'parts') and response.parts: generated_text = "".join(part.text for part in response.parts if hasattr(part, 'text'))
             if not generated_text and hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason: reason = genai.types.BlockReason(response.prompt_feedback.block_reason).name; print(f"Warning: Prompt blocked: {reason}"); generated_text = f"[Blocked: {reason}]"
             elif not generated_text and candidate is None and not hasattr(response, 'text') and not hasattr(response, 'parts'): print(f"Warning: No text in response: {response}")
             return CompletionResponse(text=generated_text)
         except Exception as e: print(f"Error calling generate_content: {e}"); raise
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse: return self._generate_sync(prompt, **kwargs)
    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:
        print("Warning: Streaming not implemented."); response = self._generate_sync(prompt, **kwargs)
        def gen() -> CompletionResponseGen: yield CompletionResponse(text=response.text, delta=response.text)
        return gen()
    async def _agenerate_sync(self, prompt: str, **kwargs) -> CompletionResponse: return self._generate_sync(prompt, **kwargs)
    async def acomplete(self, prompt: str, **kwargs: Any) -> CompletionResponse: return await self._agenerate_sync(prompt, **kwargs)
    async def astream_complete(self, prompt: str, **kwargs: Any) -> AsyncGenerator[CompletionResponse, None]:
        print("Warning: Async streaming not implemented."); response = await self._agenerate_sync(prompt, **kwargs)
        async def gen() -> AsyncGenerator[CompletionResponse, None]: yield CompletionResponse(text=response.text, delta=response.text)
        return gen()

# --- ƒê·ªãnh nghƒ©a L·ªõp Embedding T√πy ch·ªânh ---
class MyGeminiEmbedding(BaseEmbedding):
    model_name: str = "models/embedding-001"
    task_type: str = "RETRIEVAL_DOCUMENT"
    def __init__(self, model_name: str = "models/embedding-001", task_type: str = "RETRIEVAL_DOCUMENT", **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self.model_name = model_name
        if self.model_name not in available_embedding_models:
            print(f"Warning: Embedding Model '{self.model_name}' not available. Trying fallback.")
            if available_embedding_models: self.model_name = available_embedding_models[0]; print(f"Switched to: {self.model_name}")
            else: raise ValueError("No Embedding Model available.")
        self.task_type = task_type; print(f"MyGeminiEmbedding initialized: {self.model_name}")
    def _get_embedding(self, text: str, current_task_type: str) -> List[float]:
        try:
            result = genai.embed_content(model=self.model_name, content=text, task_type=current_task_type)
            if 'embedding' in result and isinstance(result['embedding'], list): return result['embedding']
            else: print(f"Error: Invalid embedding format: {result}"); raise ValueError("Invalid format")
        except Exception as e: print(f"Error getting embedding: {e}"); raise
    def _get_text_embedding(self, text: str) -> List[float]: return self._get_embedding(text, "RETRIEVAL_QUERY")
    def _get_text_embedding_batch(self, texts: List[str], show_progress: bool = False, **kwargs: Any) -> List[List[float]]:
        return [self._get_embedding(text, "RETRIEVAL_DOCUMENT") for text in texts]
    def _get_query_embedding(self, query: str) -> List[float]: return self._get_embedding(query, "RETRIEVAL_QUERY")
    async def _aget_text_embedding(self, text: str) -> List[float]: return self._get_embedding(text, "RETRIEVAL_QUERY")
    async def _aget_text_embedding_batch(self, texts: List[str], **kwargs: Any) -> List[List[float]]: return [self._get_embedding(text, "RETRIEVAL_DOCUMENT") for text in texts]
    async def _aget_query_embedding(self, query: str) -> List[float]: return self._get_embedding(query, "RETRIEVAL_QUERY")

# --- H√†m Cache ƒë·ªÉ Kh·ªüi t·∫°o Models v√† Index ---
@st.cache_resource
def load_resources():
    print("--- Loading Resources (Cache Miss or First Run) ---")
    target_llm_model, target_embed_model = None, None
    preferred_llms = ["models/gemini-1.5-flash-latest", "models/gemini-1.5-pro-latest", "models/gemini-pro"]
    preferred_embeds = ["models/embedding-001", "models/text-embedding-004"]
    for pref in preferred_llms:
        if pref in available_llm_models: target_llm_model = pref; break
    if not target_llm_model and available_llm_models: target_llm_model = available_llm_models[0]
    for pref in preferred_embeds:
        if pref in available_embedding_models: target_embed_model = pref; break
    if not target_embed_model and available_embedding_models: target_embed_model = available_embedding_models[0]
    if not target_llm_model: raise ValueError("Kh√¥ng t√¨m th·∫•y model LLM.")
    if not target_embed_model: raise ValueError("Kh√¥ng t√¨m th·∫•y model Embedding.")

    print(f"Using LLM: {target_llm_model}"); print(f"Using Embedding Model: {target_embed_model}")
    llm = MyGeminiLLM(model_name=target_llm_model)
    embed_model = MyGeminiEmbedding(model_name=target_embed_model)
    Settings.llm = llm; Settings.embed_model = embed_model
    print("LlamaIndex Settings configured.")

    DATA_DIR = "data"; index = None; documents = []
    if os.path.exists(DATA_DIR) and os.listdir(DATA_DIR):
        print(f"Loading data from '{DATA_DIR}'...")
        try:
            documents = SimpleDirectoryReader(DATA_DIR).load_data()
            if documents:
                print(f"Loaded {len(documents)} documents. Creating index...")
                # B·ªè show_progress trong cache_resource
                index = VectorStoreIndex.from_documents(documents)
                print("Index created.")
            else: print(f"No documents found in '{DATA_DIR}'.")
        except Exception as e: print(f"Error loading/indexing data: {e}")
    else: print(f"Directory '{DATA_DIR}' is empty or not found. Skipping RAG setup.")

    query_engine = index.as_query_engine() if index else None
    if query_engine: print("Query engine created.")
    else: print("Query engine not created (no index).")
    print("--- Finished Loading Resources ---")
    return query_engine, Settings.llm

# --- Kh·ªüi t·∫°o v√† L·∫•y t√†i nguy√™n t·ª´ Cache ---
try:
    query_engine, llm_global = load_resources()
except Exception as e: st.error(f"L·ªói t·∫£i t√†i nguy√™n: {e}"); st.stop()

# --- Giao di·ªán Chat ---
st.divider()
if "messages" not in st.session_state:
    welcome = "H·ªèi v·ªÅ d·ªØ li·ªáu trong 'data' nh√©." if query_engine else "Tr√≤ chuy·ªán v·ªõi Gemini nh√©."
    st.session_state.messages = [{"role": "assistant", "content": welcome}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): st.markdown(msg["content"])

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)
    with st.chat_message("assistant"):
        message_placeholder = st.empty(); message_placeholder.markdown("ü§î...")
        try:
            if query_engine:
                response = query_engine.query(prompt)
                full_response = str(response) if response is not None else "[No RAG response]"
            elif llm_global:
                 st.info("H·ªèi tr·ª±c ti·∫øp LLM...", icon="üí¨")
                 response = llm_global.complete(prompt)
                 full_response = response.text
            else: full_response = "L·ªói: Kh√¥ng c√≥ Query Engine/LLM."
            message_placeholder.markdown(full_response) # C·∫≠p nh·∫≠t c√¢u tr·∫£ l·ªùi
            st.session_state.messages.append({"role": "assistant", "content": full_response})
        except Exception as e:
            error_message = f"L·ªói: {e}"; st.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message})
            message_placeholder.markdown(error_message) # Hi·ªÉn th·ªã l·ªói