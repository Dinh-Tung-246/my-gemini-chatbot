# streamlit_app.py (Ho√†n ch·ªânh - S·ª≠a l·ªói SecretNotFound & T√πy ch·ªçn Key c·ªë ƒë·ªãnh)

import streamlit as st
import os
import google.generativeai as genai # Th∆∞ vi·ªán g·ªëc Google
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.core.llms.custom import CustomLLM # L·ªõp c∆° s·ªü cho LLM t√πy ch·ªânh
from llama_index.core.embeddings import BaseEmbedding # L·ªõp c∆° s·ªü cho Embedding t√πy ch·ªânh
from llama_index.core.llms.llm import LLM
from typing import Any, List, Optional, Sequence, AsyncGenerator
from llama_index.core.callbacks import CallbackManager
from llama_index.core.llms import CompletionResponse, CompletionResponseGen, LLMMetadata
from llama_index.core.schema import BaseNode
# from tqdm import tqdm # Kh√¥ng d√πng trong streamlit cache

# --- C·∫•u h√¨nh trang Streamlit ---
st.set_page_config(page_title="Chatbot Gemini", page_icon="‚ôä", layout="centered")
st.title("‚ôä Chatbot LlamaIndex & Google Gemini")
st.caption("ƒê∆∞·ª£c x√¢y d·ª±ng b·∫±ng LlamaIndex v√† Streamlit")

# --- Google API Key Configuration ---
# T√ôY CH·ªåN 1: ƒê·∫∑t Key tr·ª±c ti·∫øp v√†o code (CH·ªà D√ôNG ƒê·ªÇ TEST LOCAL - KH√îNG AN TO√ÄN!)
# B·ªè comment d√≤ng d∆∞·ªõi v√† thay YOUR_GOOGLE_API_KEY_HERE b·∫±ng key th·∫≠t c·ªßa b·∫°n.
# Nh·ªõ comment ho·∫∑c x√≥a d√≤ng n√†y tr∆∞·ªõc khi chia s·∫ª code ho·∫∑c deploy!
FIXED_API_KEY = "AIzaSyC4eCRIkm0MIUkBtMkZmXSe-BOrk2qySnY" # Key c·ªë ƒë·ªãnh c·ªßa b·∫°n

# T√ôY CH·ªåN 2: S·ª≠ d·ª•ng bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c Streamlit Secrets (AN TO√ÄN H∆†N)
google_api_key = None
secrets_available = hasattr(st, 'secrets') # Ki·ªÉm tra xem st.secrets c√≥ t·ªìn t·∫°i kh√¥ng

# ∆ØU TI√äN KEY C·ªê ƒê·ªäNH N·∫æU N√ì ƒê∆Ø·ª¢C ƒê·ªäNH NGHƒ®A V√Ä KH√îNG PH·∫¢I PLACEHOLDER
if 'FIXED_API_KEY' in locals() and FIXED_API_KEY and FIXED_API_KEY != "YOUR_GOOGLE_API_KEY_HERE":
    google_api_key = FIXED_API_KEY
    # Ch·ªâ hi·ªán c·∫£nh b√°o m·ªôt l·∫ßn khi d√πng key c·ªë ƒë·ªãnh
    if "fixed_key_warning_shown" not in st.session_state:
        # st.warning("ƒêang s·ª≠ d·ª•ng API Key ƒë·∫∑t c·ªë ƒë·ªãnh trong code (KH√îNG AN TO√ÄN!).", icon="‚ö†Ô∏è")
        st.session_state.fixed_key_warning_shown = True # ƒê√°nh d·∫•u ƒë√£ hi·ªÉn th·ªã
else:
    # N·∫øu kh√¥ng d√πng key c·ªë ƒë·ªãnh, th·ª≠ l·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
    google_api_key = os.environ.get("GOOGLE_API_KEY")
    if google_api_key:
        # st.info("ƒê√£ l·∫•y API Key t·ª´ bi·∫øn m√¥i tr∆∞·ªùng.", icon="üñ•Ô∏è")
        pass # ƒê√£ l·∫•y ƒë∆∞·ª£c t·ª´ env var, kh√¥ng c·∫ßn l√†m g√¨ th√™m
    elif secrets_available:
        # N·∫øu kh√¥ng c√≥ env var, m·ªõi th·ª≠ l·∫•y t·ª´ secrets (d√πng get v·ªõi default)
        google_api_key = st.secrets.get("GOOGLE_API_KEY", None)
        if google_api_key:
            st.info("ƒê√£ l·∫•y API Key t·ª´ Streamlit Secrets.", icon="üîí")

# N·∫øu sau t·∫•t c·∫£ c√°c b∆∞·ªõc tr√™n v·∫´n kh√¥ng c√≥ key, cho ph√©p nh·∫≠p
if not google_api_key:
    st.warning("Kh√¥ng t√¨m th·∫•y GOOGLE_API_KEY.")
    google_api_key = st.text_input("Nh·∫≠p Google API Key:", type="password", key="api_key_input")

# Ki·ªÉm tra l·∫ßn cu·ªëi
if not google_api_key:
    st.error("Vui l√≤ng cung c·∫•p Google API Key.", icon="üö®")
    st.stop() # D·ª´ng ·ª©ng d·ª•ng n·∫øu kh√¥ng c√≥ key

# --- C·∫•u h√¨nh th∆∞ vi·ªán Google ---
try:
    genai.configure(api_key=google_api_key)
except Exception as e:
    st.error(f"L·ªói c·∫•u h√¨nh th∆∞ vi·ªán Google: {e}", icon="üî•")
    st.stop()

# --- Cache Models v√† Kh·ªüi t·∫°o ---
@st.cache_data
def load_available_models():
    llm_models, embed_models = [], []
    try:
        # st.info("ƒêang ki·ªÉm tra models kh·∫£ d·ª•ng...") # B·ªè b·ªõt th√¥ng b√°o
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods: llm_models.append(m.name)
            if 'embedContent' in m.supported_generation_methods: embed_models.append(m.name)
        # st.success("Ki·ªÉm tra model ho√†n t·∫•t.")
    except Exception as e:
        st.warning(f"Kh√¥ng th·ªÉ li·ªát k√™ models: {e}. D√πng m·∫∑c ƒë·ªãnh.")
        llm_models, embed_models = ["models/gemini-1.5-flash-latest"], ["models/embedding-001"]
    # In ra console ƒë·ªÉ debug (kh√¥ng hi·ªÉn th·ªã tr√™n UI)
    print(f"Available LLM models: {llm_models}")
    print(f"Available Embedding models: {embed_models}")
    return llm_models, embed_models

available_llm_models, available_embedding_models = load_available_models()

# --- ƒê·ªãnh nghƒ©a L·ªõp LLM T√πy ch·ªânh ---
class MyGeminiLLM(CustomLLM):
    model_name: str = "models/gemini-1.5-flash-latest"
    _model: Any = None
    def __init__(self, model_name: str = "models/gemini-1.5-flash-latest", **kwargs) -> None:
        super().__init__(**kwargs)
        self.model_name = model_name
        try:
            if self.model_name not in available_llm_models:
                 print(f"Warning: Model '{self.model_name}' not in available LLMs {available_llm_models}. Trying fallback.")
                 if available_llm_models: self.model_name = available_llm_models[0]; print(f"Switched to: {self.model_name}")
                 else: raise ValueError("No fallback LLM available.")
            self._model = genai.GenerativeModel(self.model_name)
            print(f"MyGeminiLLM initialized with model: {self.model_name}")
        except Exception as e: print(f"Error initializing LLM '{self.model_name}': {e}"); raise
    @property
    def metadata(self) -> LLMMetadata:
        context_window = 1048576 if "1.5" in self.model_name else 30720
        return LLMMetadata(context_window=context_window, num_output=8192, model_name=self.model_name, is_chat_model=True)
    def _generate_sync(self, prompt: str, **kwargs) -> CompletionResponse:
         try:
             safety_settings=[{"category": c, "threshold": "BLOCK_NONE"} for c in ["HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH", "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT"]]
             response = self._model.generate_content(prompt, safety_settings=safety_settings)
             generated_text = ""; candidate = None
             if hasattr(response, 'candidates') and response.candidates: candidate = response.candidates[0]
             if candidate and candidate.finish_reason == 1 and hasattr(candidate.content, 'parts') and candidate.content.parts: generated_text = "".join(part.text for part in candidate.content.parts if hasattr(part, 'text'))
             elif candidate and candidate.finish_reason != 0: reason = genai.types.Candidate.FinishReason(candidate.finish_reason).name; print(f"Warning: Response blocked: {reason}"); generated_text = f"[Blocked: {reason}]"
             elif hasattr(response, 'text'): generated_text = response.text
             elif hasattr(response, 'parts') and response.parts: generated_text = "".join(part.text for part in response.parts if hasattr(part, 'text'))
             if not generated_text and hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason: reason = genai.types.BlockReason(response.prompt_feedback.block_reason).name; print(f"Warning: Prompt blocked: {reason}"); generated_text = f"[Blocked: {reason}]"
             elif not generated_text and candidate is None and not hasattr(response, 'text') and not hasattr(response, 'parts'): print(f"Warning: No text in response: {response}")
             return CompletionResponse(text=generated_text)
         except Exception as e: print(f"Error calling generate_content: {e}"); raise
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse: return self._generate_sync(prompt, **kwargs)
    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:
        print("Warning: Streaming not implemented."); response = self._generate_sync(prompt, **kwargs)
        def gen() -> CompletionResponseGen: yield CompletionResponse(text=response.text, delta=response.text)
        return gen()
    async def _agenerate_sync(self, prompt: str, **kwargs) -> CompletionResponse: return self._generate_sync(prompt, **kwargs)
    async def acomplete(self, prompt: str, **kwargs: Any) -> CompletionResponse: return await self._agenerate_sync(prompt, **kwargs)
    async def astream_complete(self, prompt: str, **kwargs: Any) -> AsyncGenerator[CompletionResponse, None]:
        print("Warning: Async streaming not implemented."); response = await self._agenerate_sync(prompt, **kwargs)
        async def gen() -> AsyncGenerator[CompletionResponse, None]: yield CompletionResponse(text=response.text, delta=response.text)
        return gen()

# --- ƒê·ªãnh nghƒ©a L·ªõp Embedding T√πy ch·ªânh ---
class MyGeminiEmbedding(BaseEmbedding):
    model_name: str = "models/embedding-001"
    task_type: str = "RETRIEVAL_DOCUMENT"
    def __init__(self, model_name: str = "models/embedding-001", task_type: str = "RETRIEVAL_DOCUMENT", **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self.model_name = model_name
        if self.model_name not in available_embedding_models:
            print(f"Warning: Embedding Model '{self.model_name}' not available. Trying fallback.")
            if available_embedding_models: self.model_name = available_embedding_models[0]; print(f"Switched to: {self.model_name}")
            else: raise ValueError("No Embedding Model available.")
        self.task_type = task_type; print(f"MyGeminiEmbedding initialized: {self.model_name}")
    def _get_embedding(self, text: str, current_task_type: str) -> List[float]:
        try:
            result = genai.embed_content(model=self.model_name, content=text, task_type=current_task_type)
            if 'embedding' in result and isinstance(result['embedding'], list): return result['embedding']
            else: print(f"Error: Invalid embedding format: {result}"); raise ValueError("Invalid format")
        except Exception as e: print(f"Error getting embedding: {e}"); raise
    def _get_text_embedding(self, text: str) -> List[float]: return self._get_embedding(text, "RETRIEVAL_QUERY")
    def _get_text_embedding_batch(self, texts: List[str], show_progress: bool = False, **kwargs: Any) -> List[List[float]]:
        # B·ªè tqdm ƒë·ªÉ tr√°nh l·ªói v·ªõi cache_resource
        return [self._get_embedding(text, "RETRIEVAL_DOCUMENT") for text in texts]
    def _get_query_embedding(self, query: str) -> List[float]: return self._get_embedding(query, "RETRIEVAL_QUERY")
    async def _aget_text_embedding(self, text: str) -> List[float]: return self._get_embedding(text, "RETRIEVAL_QUERY")
    async def _aget_text_embedding_batch(self, texts: List[str], **kwargs: Any) -> List[List[float]]: return [self._get_embedding(text, "RETRIEVAL_DOCUMENT") for text in texts]
    async def _aget_query_embedding(self, query: str) -> List[float]: return self._get_embedding(query, "RETRIEVAL_QUERY")

# --- H√†m Cache ƒë·ªÉ Kh·ªüi t·∫°o Models v√† Index ---
@st.cache_resource # Cache ƒë·ªÉ kh√¥ng kh·ªüi t·∫°o l·∫°i LLM/Embed/Index m·ªói l·∫ßn rerun
def load_resources():
    """Kh·ªüi t·∫°o LLM, Embed Model, n·∫°p d·ªØ li·ªáu v√† t·∫°o Index."""
    print("--- Loading Resources (Cache Miss or First Run) ---") # In ra ƒë·ªÉ bi·∫øt khi n√†o cache ch·∫°y
    # Configure Settings
    target_llm_model, target_embed_model = None, None
    preferred_llms = ["models/gemini-1.5-flash-latest", "models/gemini-1.5-pro-latest", "models/gemini-pro"]
    preferred_embeds = ["models/embedding-001", "models/text-embedding-004"] # Th√™m model m·ªõi c·ªßa Google n·∫øu c√≥
    for pref in preferred_llms:
        if pref in available_llm_models: target_llm_model = pref; break
    if not target_llm_model and available_llm_models: target_llm_model = available_llm_models[0]
    for pref in preferred_embeds:
        if pref in available_embedding_models: target_embed_model = pref; break
    if not target_embed_model and available_embedding_models: target_embed_model = available_embedding_models[0]
    if not target_llm_model: raise ValueError("Kh√¥ng t√¨m th·∫•y model LLM.")
    if not target_embed_model: raise ValueError("Kh√¥ng t√¨m th·∫•y model Embedding.")

    print(f"Using LLM: {target_llm_model}"); print(f"Using Embedding Model: {target_embed_model}")
    llm = MyGeminiLLM(model_name=target_llm_model)
    embed_model = MyGeminiEmbedding(model_name=target_embed_model)
    Settings.llm = llm; Settings.embed_model = embed_model
    print("LlamaIndex Settings configured.")

    # Load Data and Create Index
    DATA_DIR = "data"; index = None; documents = []
    if os.path.exists(DATA_DIR) and os.listdir(DATA_DIR):
        print(f"Loading data from '{DATA_DIR}'...")
        try:
            documents = SimpleDirectoryReader(DATA_DIR).load_data()
            if documents:
                print(f"Loaded {len(documents)} documents. Creating index...")
                index = VectorStoreIndex.from_documents(documents)
                print("Index created.")
            else: print(f"No documents found in '{DATA_DIR}'.")
        except Exception as e: print(f"Error loading/indexing data: {e}")
    else: print(f"Directory '{DATA_DIR}' is empty or not found. Skipping RAG setup.")

    # Create Query Engine if index exists
    query_engine = index.as_query_engine() if index else None
    if query_engine: print("Query engine created.")
    else: print("Query engine not created (no index).")

    print("--- Finished Loading Resources ---")
    return query_engine, Settings.llm # Tr·∫£ v·ªÅ engine v√† llm ƒë·ªÉ d√πng

# --- Kh·ªüi t·∫°o v√† L·∫•y t√†i nguy√™n t·ª´ Cache ---
try:
    query_engine, llm_global = load_resources()
except Exception as e:
    st.error(f"L·ªói nghi√™m tr·ªçng khi t·∫£i t√†i nguy√™n: {e}")
    st.stop()

# --- Giao di·ªán Chat ---
st.divider()
if "messages" not in st.session_state:
    welcome = "H·ªèi v·ªÅ d·ªØ li·ªáu trong 'data' nh√©." if query_engine else "Tr√≤ chuy·ªán v·ªõi Gemini nh√© (kh√¥ng c√≥ d·ªØ li·ªáu n·ªÅn)."
    st.session_state.messages = [{"role": "assistant", "content": welcome}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): st.markdown(msg["content"])

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)
    with st.chat_message("assistant"):
        message_placeholder = st.empty(); message_placeholder.markdown("ü§î...")
        try:
            if query_engine:
                response = query_engine.query(prompt)
                full_response = str(response) if response is not None else "[No RAG response]"
                message_placeholder.markdown(full_response)
            elif llm_global: # S·ª≠ d·ª•ng llm ƒë√£ l·∫•y t·ª´ cache
                 st.info("H·ªèi tr·ª±c ti·∫øp LLM...", icon="üí¨")
                 response = llm_global.complete(prompt)
                 full_response = response.text
                 message_placeholder.markdown(full_response)
            else: full_response = "L·ªói: Kh√¥ng c√≥ Query Engine/LLM."; message_placeholder.error(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
        except Exception as e:
            error_message = f"L·ªói: {e}"; st.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message})
            message_placeholder.markdown(error_message)