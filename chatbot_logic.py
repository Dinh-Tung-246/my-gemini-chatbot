# chatbot_logic.py
import streamlit as st
import google.generativeai as genai
import google.ai.generativelanguage as glm

@st.cache_resource
def load_gemini_model(model_name):
    """T·∫£i v√† cache model Generative AI."""
    try:
        # st.info(f"ƒêang t·∫£i model: {model_name}...")
        model = genai.GenerativeModel(model_name)
        # st.success(f"ƒê√£ t·∫£i th√†nh c√¥ng model: {model_name}")
        # C·∫≠p nh·∫≠t model ƒëang ƒë∆∞·ª£c cache trong session_state
        st.session_state.current_model_name = model_name
        return model
    except Exception as e:
        st.error(f"‚ùå L·ªói nghi√™m tr·ªçng khi t·∫£i model {model_name}: {e}")
        st.stop()

def initialize_chat_session(model):
    """
    Kh·ªüi t·∫°o ho·∫∑c t·∫£i l·∫°i ChatSession n·∫øu c·∫ßn (v√≠ d·ª•: model thay ƒë·ªïi).
    C·∫≠p nh·∫≠t st.session_state.chat v√† st.session_state.messages n·∫øu c·∫ßn.
    """
    model_name_selected = st.session_state.get('selected_model', None)
    current_model_loaded = st.session_state.get('current_model_name', None)
    chat_session_exists = "chat" in st.session_state and st.session_state.chat is not None

    # Ki·ªÉm tra c√°c ƒëi·ªÅu ki·ªán c·∫ßn kh·ªüi t·∫°o l·∫°i chat
    model_changed = current_model_loaded != model_name_selected
    needs_reinitialization = not chat_session_exists or model_changed

    if needs_reinitialization:
        if model_changed and chat_session_exists:
            st.warning(f"Model ƒë√£ thay ƒë·ªïi. B·∫Øt ƒë·∫ßu cu·ªôc tr√≤ chuy·ªán m·ªõi v·ªõi {model_name_selected}.")
            st.session_state.messages = [] # X√≥a l·ªãch s·ª≠ khi ƒë·ªïi model
            st.session_state.chat = None # X√≥a session c≈©

        # Kh·ªüi t·∫°o tin nh·∫Øn ch√†o m·ª´ng n·∫øu messages r·ªóng ho·∫∑c v·ª´a b·ªã x√≥a
        if not st.session_state.get("messages"):
             model_short_name = model_name_selected.split('/')[-1]
             st.session_state.messages = [{
                 "role": "model",
                 "content": f"ü§ñ Xin ch√†o! M√¨nh l√† **{model_short_name}**. B·∫°n mu·ªën h·ªèi g√¨?"
             }]

        # Chuy·ªÉn ƒë·ªïi l·ªãch s·ª≠ hi·ªán c√≥ (tr·ª´ tin nh·∫Øn cu·ªëi c√πng) sang ƒë·ªãnh d·∫°ng Google AI
        google_history = []
        if st.session_state.get("messages"):
            for msg in st.session_state.messages[:-1]:
                if msg.get("content"):
                    role_google = "user" if msg["role"] == "user" else "model"
                    try:
                         # S·ª≠ d·ª•ng class Content v√† Part t·ª´ glm
                         google_history.append(glm.Content(role=role_google, parts=[glm.Part(text=msg["content"])]))
                    except Exception as e:
                        st.warning(f"B·ªè qua tin nh·∫Øn kh√¥ng h·ª£p l·ªá khi t·∫°o history: {msg}. L·ªói: {e}")

        # Kh·ªüi t·∫°o chat session m·ªõi
        try:
            st.session_state.chat = model.start_chat(history=google_history)
            # C·∫≠p nh·∫≠t l·∫°i model ƒëang d√πng c·ªßa session n√†y
            st.session_state.current_model_name = model_name_selected
            # st.info("Chat session ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o/t·∫£i l·∫°i.")
        except Exception as e:
            st.error(f"‚ùå L·ªói nghi√™m tr·ªçng khi b·∫Øt ƒë·∫ßu chat session: {e}")
            st.stop()
    # Tr·∫£ v·ªÅ chat session hi·ªán t·∫°i (d√π m·ªõi t·∫°o hay ƒë√£ c√≥)
    return st.session_state.chat


def generate_response_stream(chat_session, prompt, generation_config, safety_settings):
    """
    G·ª≠i prompt ƒë·∫øn chat session v√† tr·∫£ v·ªÅ m·ªôt generator cho response stream.
    X·ª≠ l√Ω c√°c l·ªói API c·ª• th·ªÉ trong qu√° tr√¨nh g·ª≠i.
    """
    try:
        response_stream = chat_session.send_message(
            prompt,
            stream=True,
            safety_settings=safety_settings,
            generation_config=generation_config
        )
        # Tr·∫£ v·ªÅ generator ƒë·ªÉ x·ª≠ l√Ω b√™n ngo√†i
        for chunk in response_stream:
            yield chunk

    except genai.types.BlockedPromptException as e:
        st.error(f"üö´ Y√™u c·∫ßu b·ªã ch·∫∑n: {e}")
        yield f"‚ö†Ô∏è **L·ªói:** Y√™u c·∫ßu c·ªßa b·∫°n ƒë√£ b·ªã ch·∫∑n b·ªüi b·ªô l·ªçc n·ªôi dung." # Tr·∫£ v·ªÅ th√¥ng b√°o l·ªói nh∆∞ m·ªôt chunk
    except genai.types.StopCandidateException as e:
        st.error(f"‚ö†Ô∏è Ph·∫£n h·ªìi b·ªã d·ª´ng: {e}")
        yield f"‚ö†Ô∏è **L·ªói:** Ph·∫£n h·ªìi b·ªã d·ª´ng ƒë·ªôt ng·ªôt b·ªüi API."
    except Exception as e:
        st.error(f"‚ùå L·ªói khi g·ªçi API: {e}")
        yield f"‚ö†Ô∏è **L·ªói:** ƒê√£ c√≥ l·ªói x·∫£y ra khi giao ti·∫øp v·ªõi Gemini API."

def get_blocked_reason(chat_session):
    """C·ªë g·∫Øng l·∫•y l√Ω do b·ªã ch·∫∑n t·ª´ ph·∫£n h·ªìi cu·ªëi c√πng c·ªßa chat session."""
    try:
        last_response = chat_session.last_response
        if last_response and last_response.prompt_feedback:
            return last_response.prompt_feedback.block_reason.name
    except Exception:
        pass # B·ªè qua n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c
    return "Kh√¥ng r√µ l√Ω do"